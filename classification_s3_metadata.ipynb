{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classifying S3 Data Sensitivity with Machine Learning\n",
    "This notebook demonstrates how to categorize S3 data objects as sensitive or non-sensitive by analyzing object metadata with Python and scikit-learn.\n",
    "\n",
    "## 1. Business Problem\n",
    "With data stored in S3 buckets, we need an automated way to identify sensitive data and enforce security policies. Manually classifying data does not scale.\n",
    "\n",
    "We will build a proof of concept to show how object metadata like bucket names and access patterns can be used to train an ML model to classify sensitivity.\n",
    "\n",
    "## 2. Sample Data\n",
    "We create sample metadata for a few S3 objects containing attributes like bucket name and last accessed date:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b2304e3b7dcb92"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sns\n",
    "\n",
    "data = [{'s3bucket': 'financial-data', 'days_since_access': 2291, 'data_size_gb': 0.02},\n",
    "        {'s3bucket': 'model-data', 'days_since_access': 119, 'data_size_gb': 0.02},\n",
    "        {'s3bucket': 'log-files', 'days_since_access': 2733, 'data_size_gb': 0.01},\n",
    "        {'s3bucket': 'airflow-genie', 'days_since_access': 2291, 'data_size_gb': 0.02},\n",
    "        {'s3bucket': 'searchmetadata', 'days_since_access': 119, 'data_size_gb': 0.02},\n",
    "        {'s3bucket': 'digital-archive-east', 'days_since_access': 138, 'data_size_gb': 15.92}]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "display(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72779e47d1bd80bc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Feature Engineering\n",
    "We transform the text data about S3 bucket names into numeric vectors using scikit-learn's TfidfVectorizer. This encoder converts text into tf-idf vectors.\n",
    "\n",
    "TfidfVectorizer removes stopwords, applies tokenization, ngram generation, and calculates document frequencies to encode text data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba40e4742cd269fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = df.plot.bar(x='s3bucket', y='days_since_access', rot=0)\n",
    "ax.set_ylabel(\"Days Since Last Access\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd88de9d066fc836",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below heatmap visualization shows the **tf-idf vector values** for each feature (token) from the `TfidfVectorizer` text encoding process.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "* The x-axis contains the **feature names** - these are the **individual text tokens** (terms) extracted from the S3 bucket names after preprocessing (lowercasing, stopword removal etc).\n",
    "\n",
    "* The y-axis corresponds to the **individual S3 records** from our sample metadata.\n",
    "\n",
    "* Each cell shows the **tf-idf weight** calculated by the vectorizer for that particular token in that specific document.\n",
    "\n",
    "* **Brighter colors** indicate a **higher tf-idf value** and hence a greater relevance for that term in the document.\n",
    "\n",
    "So in summary, the heatmap gives us:\n",
    "\n",
    "- A glance of the most \"important\" words in characterizing each document based on word frequencies.\n",
    "\n",
    "- This is a vectorized numeric representation of the original text data.\n",
    "\n",
    "- Shows which terms distinguish documents from one another.\n",
    "\n",
    "Analyzing these high-weight features can give insight into patterns that help predict sensitivity - e.g financial tokens being more indicative of confidential data.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a9d7437681db78d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['s3bucket'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "ax = sns.heatmap(X.toarray(), xticklabels=feature_names)\n",
    "plt.xlabel(\"Tokens\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b125f957fde54cf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Classification Model\n",
    "\n",
    "With numeric vectors representing the S3 metadata, we can now train a classification algorithm to predict sensitivity labels. \n",
    "\n",
    "For this, we will use the **Multinomial Naive Bayes** algorithm. \n",
    "\n",
    "#### Overview\n",
    "\n",
    "The Multinomial NB model is a common choice for document classification tasks. It applies Bayes' theorem with a strong assumption of **word independence** to calculate probabilities of classes.\n",
    "\n",
    "The advantages are that it is **simple, fast to train, and works well for text data.**\n",
    "\n",
    "#### Mathematical Details\n",
    "\n",
    "- Uses multinomial distributions to represent word counts\n",
    "- Predicts classes using Bayes' rule to calculate posterior probabilities:\n",
    "\n",
    "$\\Large{P(c_k|d)=\\frac{P(c_k)\\prod_{i=1}^{N_{words}} P(w_i|c_k)^{n_{i}}}{P(d)}}$\n",
    "\n",
    "where $n_{i,j}$ is the number of times word $w_i$ appeared in training document $d_j$.\n",
    "\n",
    "We implement the model in Python as:  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8874f7e623121cf3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Vectorize text\n",
    "X = vectorizer.fit_transform(df['s3bucket'])\n",
    "\n",
    "# Labels\n",
    "y = [0, 1, 0, 0, 1, 0]\n",
    "\n",
    "# Instantiate NB model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train model\n",
    "nb.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43396f647f7b915a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Making Predictions\n",
    "We can now use our model to classify new S3 objects:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e30f9cfc09fffc0c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(['financial-reports'])\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "print(y_pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2c14d1e26c2bc14",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Evaluating Performance\n",
    "We check accuracy on sample data by comparing to known labels:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82f0615b07e47e2f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y = np.array([0, 1, 0])\n",
    "y_pred = np.array([0, 1, 1])\n",
    "\n",
    "accuracy_score(y, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9473cac3137ecfc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In real applications, precision and recall also matter for sensitive data.\n",
    "\n",
    "This notebooks shows a basic workflow for metadata-based S3 classification with Python. Next steps could include larger data, better features, and tuning models.\n",
    "\n",
    "## XGBoost for Enhanced Predictions\n",
    "### Overview\n",
    "In addition to Naive Bayes, we will incorporate XGBoost - an extremely popular tree boosting algorithm - to classify S3 object sensitivity.\n",
    "\n",
    "Some benefits of using XGBoost include:\n",
    "\n",
    "- Handles raw features beyond text vectors\n",
    "- Operates by ensembling weak decision tree models\n",
    "- Less prone to overfitting compared to deep nets\n",
    "\n",
    "### Mathematical Details\n",
    "XGBoost builds an additive regression model in a forward stage-wise fashion:\n",
    "\n",
    "$y_{i} = \\phi(x_i) = \\sum_{k=1}^{K} f_k(x_i)$\n",
    "\n",
    "Where $f_k$ are the individual decision trees.\n",
    "\n",
    "\n",
    "Trees are built sequentially to minimize the following regularized objective:\n",
    "\n",
    "$\\sum_{i=1}^{n} l(\\hat{y_i}, y_i) + \\sum_{k=1}^{K}\\Omega(f_k)$\n",
    "\n",
    "Where $l$ is a differentiable convex loss function and $\\Omega$ penalizes model complexity.\n",
    "\n",
    "### Implementing XGBoost Classifier\n",
    "\n",
    "With the Naive Bayes model defined, we can enhance predictions by adding a powerful XGBoost Classifier and ensembling outputs.\n",
    "\n",
    "Some benefits of using XGBoost include:\n",
    "\n",
    "- Works well with raw numeric features \n",
    "- Operates by ensembling decision tree models\n",
    "- Generalizes better than deep learning\n",
    "\n",
    "To properly integrate it with our workflow:\n",
    "\n",
    "**1. Balance Data Samples**\n",
    "\n",
    "We first ensure `X` and `y` have the *same number of samples*:\n",
    "\n",
    "```\n",
    "Length of X: 6  \n",
    "Length of y: 6\n",
    "``` \n",
    "\n",
    "This avoids downstream errors in splitting or fitting.\n",
    "\n",
    "**2. Split Data**\n",
    "\n",
    "We use scikit-learn's `train_test_split` to create train and test sets:   \n",
    "\n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) \n",
    "```\n",
    "\n",
    "**3. Fit Both Models**\n",
    "\n",
    "The NB and XGB models are trained on the same `X_train` and `y_train` sets:\n",
    "\n",
    "```\n",
    "nb.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)  \n",
    "```\n",
    "\n",
    "This captures different aspects of the data.\n",
    "\n",
    "\n",
    "**4. Ensemble Predictions** \n",
    "\n",
    "We average predictions from both models on `X_test` to get our final output:\n",
    "\n",
    "```\n",
    "if xgb_pred[i] == nb_pred[i]:\n",
    "     ensemble_pred.append(xgb_pred[i]) \n",
    "```\n",
    "\n",
    "Ensembling reduces variance and improves robustness.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "597624ad3a0beaac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Ensure X is properly defined and vectorized here\n",
    "\n",
    "# Adjust y to match the length of X\n",
    "y = [0, 1, 0, 0, 1, 0]  # This should match the length of X\n",
    "\n",
    "# Verify that X and y have the same length\n",
    "print(\"Length of X:\", X.shape[0])\n",
    "print(\"Length of y:\", len(y))\n",
    "\n",
    "# Proceed with the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train NB model\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Train XGB model  \n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "nb_pred = nb.predict(X_test)\n",
    "\n",
    "# Ensemble predictions\n",
    "ensemble_pred = []\n",
    "num_test_samples = len(y_test)  # Number of test samples\n",
    "for i in range(num_test_samples):\n",
    "        if xgb_pred[i] == nb_pred[i]:\n",
    "                ensemble_pred.append(xgb_pred[i])\n",
    "        else:\n",
    "                ensemble_pred.append(1)  # Replace with a suitable default or decision rule\n",
    "\n",
    "print(ensemble_pred)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ad21edc141c0bf8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "The key output from ensembling the XGBoost and Naive Bayes models is the `ensemble_pred` array:\n",
    "\n",
    "```python\n",
    "print(ensemble_pred)\n",
    "# [1, 1] \n",
    "```\n",
    "\n",
    "This output contains the final sensitivity predictions on the test set after ensembling. \n",
    "\n",
    "Some key points on interpreting it:\n",
    "\n",
    "- It is an array of 0s and 1s representing predicted labels\n",
    "- Where 0 = Non Sensitive, 1 = Sensitive\n",
    "- Each index maps to an S3 metadata record in X_test\n",
    "\n",
    "So for the example output:\n",
    "\n",
    "- We have 2 test records (length is 2)\n",
    "- Both records are classified as sensitive (label 1)\n",
    "\n",
    "The models both independently predicted the samples as sensitive, so the ensemble keeps the agreed predictions. \n",
    "\n",
    "In cases where NB and XGB disagree on a prediction:\n",
    "\n",
    "```\n",
    "if xgb_pred[i] != nb_pred[i]:\n",
    "     ensemble_pred.append(1)  \n",
    "```\n",
    "\n",
    "We have used a simple tie-break rule of choosing sensitive by default. But this could be customized.\n",
    "\n",
    "In summary, the output is an array of ensemble predictions on unseen test data. We have confidence in results where both models agree."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c035a07aa62e6145"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Generating Contextual Names with OpenAI \n",
    "\n",
    "So far we have classified S3 objects into generic `sensitive` and `non-sensitive` categories based on metadata like bucket names and access recency.\n",
    "\n",
    "But we can enhance interpretability by using OpenAI to generate meaningful, customized names reflecting the metadata for each predicted class.\n",
    "\n",
    "For example, instead of just `sensitive`, names like:\n",
    "\n",
    "- `aged_financials` \n",
    "- `inactive_logs`\n",
    "- `recent_models`\n",
    "\n",
    "### Intuition\n",
    "\n",
    "This provides more insight into *why* certain records are deemed sensitive by the models based on their attributes.\n",
    "\n",
    "### Implementation \n",
    "\n",
    "We define a function that takes an S3 bucket name and days since last access as input, and uses the OpenAI text generation API to suggest contextual names:\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "def generate_name(bucket, days):\n",
    "        prompt = f\"Suggest a classification name for an S3 bucket named {bucket} with {days} days since last access.\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", \n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "```\n",
    "\n",
    "We then call this function on sample metadata to print name ideas:\n",
    "\n",
    "```python\n",
    "bucket_name = \"financial-data\"\n",
    "days_since_access = 2291\n",
    "\n",
    "name1 = generate_name(bucket_name, days_since_access)\n",
    "print(name1)\n",
    "```\n",
    "\n",
    "And use the names generated instead of generic labels when printing predictions or visualization.\n",
    "\n",
    "This small addition of using OpenAI for labeling provides much more transparency into the model's sensitivity designations. We can evalute if the contextual names seem appropriate, or re-train if certain labels are incorrect or misleading.\n",
    "\n",
    "Let me know if you have any other questions!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a78c1cbea7daac0f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"your key\"\n",
    "def generate_name(bucket, days):\n",
    "        prompt = f\"Suggest a classification name for an S3 bucket named {bucket} with {days} days since last access.\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "\n",
    "bucket_name = \"financial-data\"\n",
    "days_since_access = 2291\n",
    "\n",
    "name1 = generate_name(bucket_name, days_since_access)\n",
    "print(name1)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "876d5c3759ee5436",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Customized Naming with Hugging Face\n",
    "\n",
    "So far we classified S3 objects into generic categories like `sensitive` and `non-sensitive`. \n",
    "\n",
    "To improve interpretability, we can **customize names reflecting metadata** like so:\n",
    "\n",
    "- `aged_financials`\n",
    "- `inactive_logs`  \n",
    "\n",
    "This provides more insight into why records match specific classifications.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "We leverage Hugging Face's pretrained models to generate names:\n",
    "\n",
    "```python\n",
    "text_generator = pipeline(\"text-generation\") \n",
    "```\n",
    "\n",
    "Our naming function takes metadata and produces name ideas:\n",
    "\n",
    "```python\n",
    "def generate_name(bucket, days):\n",
    "\n",
    "    prompt = f\"Generate name for S3 bucket {bucket} with {days} days since access\"\n",
    "    \n",
    "    outputs = text_generator(prompt)  \n",
    "    return outputs[0][\"generated_text\"]\n",
    "```\n",
    "\n",
    "For example for financial data inactive for years:\n",
    "\n",
    "``` \n",
    "bucket_name = \"financial-data\"\n",
    "days_since_access = 2291\n",
    "\n",
    "name = generate_name(bucket_name, days_since_access) \n",
    "# \"aged_financials\"\n",
    "```\n",
    "\n",
    "We can now print or display these customized names instead of just `sensitive` labels.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5acfe577c2e83f1e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip uninstall packaging\n",
    "# !!pip install packaging"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56da2cae05fe4f62",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "\n",
    "def generate_name(bucket, days):\n",
    "        prompt = f\"Generate name for S3 bucket {bucket} with {days} days since access\"\n",
    "        outputs = text_generator(prompt, do_sample=True, min_length=5, max_length=20)\n",
    "        return outputs[0][\"generated_text\"]\n",
    "\n",
    "bucket_name = \"financial-data\"\n",
    "days_since_access = 2291\n",
    "\n",
    "name = generate_name(bucket_name, days_since_access)\n",
    "print(name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5da998948b39c7d9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c5292b770051209c",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
